---
layout: algorithm
title: Machine Learning (Andrew Ng, Stanford University)
comments: true
mathjax: false
---

# Machine Learning
by Professor Andrew Ng, Stanford University

<br>

## Video Lectures
[https://pan.baidu.com/s/1nuFcdip](https://pan.baidu.com/s/1nuFcdip)<br>
Password: ui3s

<br>

## Notes

+ Lecture 1 - Introduction &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/1.pdf)]
  - 1.1 Welcome
  - 1.2 What is Machine Learning
  - 1.3 Supervised Learning
  - 1.4 Unsupervised Learning
+ Lecture 2 - Linear Regression with One Variable &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/2.pdf)]
  - 2.1 Model Representation
  - 2.2 Cost Function
  - 2.3 Cost Function - Intuition I
  - 2.4 Cost Function - Intuition II
  - 2.5 Gradient Descent
  - 2.6 Gradient Descent - Intuition
  - 2.7 Gradient Descent for Linear Regression
  - 2.8 What's Next?
+ Lecture 3 - Linear Algebra Review (Optional) &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/3.pdf)]
  - 3.1 Matrices and Vectors
  - 3.2 Addition and Scalar Multiplication
  - 3.3 Matrix - Vector Multiplication
  - 3.4 Matrix - Matrix Multiplication
  - 3.5 Matrix Multiplication Properties
  - 3.6 Inverse and Transpose
+ Lecture 4 - Linear Regression with Multiple Variables &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/4.pdf)]
  - 4.1 Multiple Features
  - 4.2 Gradient Descent for Multiple Variables
  - 4.3 Gradient Descent in Practice I - Feature Scaling
  - 4.4 Gradient Descent in Practice II - Learning Rate
  - 4.5 Features and Polynomial Regression
  - 4.6 Normal Equation
  - 4.7 Normal Equation - Non-Invertibility (Optional)
+ Lecture 5 - Octave Tutorial &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/5.pdf)]
  - 5.1 Basic Operations
  - 5.2 Moving Data Around
  - 5.3 Computing on Data
  - 5.4 Plotting Data
  - 5.5 Control Statements - for, while, if statements
  - 5.6 Vectorization
  - 5.7 Working on and Submitting Programming Exercises
+ Lecture 6 - Logistic Regression &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/6.pdf)]
  - 6.1 Classification
  - 6.2 Hypothesis Representation
  - 6.3 Decision Boundary
  - 6.4 Cost Function
  - 6.5 Simplified Cost Function and Gradient Descent
  - 6.6 Advanced Optimization
  - 6.7 Multiclass Classification
+ Lecture 7 - Regularization &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/7.pdf)]
  - 7.1 The Problem of Overfitting
  - 7.2 Cost Function
  - 7.3 Regularized Linear Regression
  - 7.4 Regularized Logistic Regression
+ Lecture 8 - Neural Networks - Representation &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/8.pdf)]
  - 8.1 Non-Linear Hypotheses
  - 8.2 Neurons and the Brain
  - 8.3 Model Representation I
  - 8.4 Model Representation II
  - 8.5 Examples and Intuitions I
  - 8.6 Examples and Intuitions II
  - 8.7 Multiclass Classification
+ Lecture 9 - Neural Networks - Learning &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/9.pdf)]
  - 9.1 Cost Function
  - 9.2 Backpropagation - Algorithm
  - 9.3 Backpropagation - Intuition
  - 9.4 Implementation Note - Unrolling Parameters
  - 9.5 Gradient Checking
  - 9.6 Random Initialization
  - 9.7 Putting it Together
  - 9.8 Autonomous Driving
+ Lecture 10 - Advice for Applying Machine Learning &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/10.pdf)]
  - 10.1 Deciding What to Try Next
  - 10.2 Evaluating a Hypothesis
  - 10.3 Model Selection and Train-Validation-Test Sets
  - 10.4 Diagnosing Bias vs. Variance
  - 10.5 Regularization and Bias-Variance
  - 10.6 Learning Curves
  - 10.7 Deciding What to Do Next Revisited
+ Lecture 11 - Machine Learning System Design &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/11.pdf)]
  - 11.1 Prioritizing What to Work On
  - 11.2 Error Analysis
  - 11.3 Error Metrics for Skewed Classes
  - 11.4 Trading Off Precision and Recall
  - 11.5 Data For Machine Learning
+ Lecture 12 - Support Vector Machines &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/12.pdf)]
  - 12.1 Optimization Objective
  - 12.2 Large Margin Intuition
  - 12.3 Mathematics Behind Large Margin Classification (Optional)
  - 12.4 Kernels I
  - 12.5 Kernels II
  - 12.6 Using an SVM
+ Lecture 13 - Clustering &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/13.pdf)]
  - 13.1 Unsupervised Learning - Introduction
  - 13.2 K-Means Algorithm
  - 13.3 Optimization Objective
  - 13.4 Random Initialization
  - 13.5 Choosing the Number of Clusters
+ Lecture 14 - Dimension Reduction &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/14.pdf)]
  - 14.1 Motivation I - Data Compression
  - 14.2 Motivation II - Visualization
  - 14.3 Principal Component Analysis - Problem Formulation
  - 14.4 Principal Component Analysis - Algorithm
  - 14.5 Choosing the Number of Principal Components
  - 14.6 Reconstruction from Compressed Representation
  - 14.7 Advice for Applying PCA
+ Lecture 15 - Anomaly Detection &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/15.pdf)]
  - 15.1 Problem Motivation
  - 15.2 Gaussian Distribution
  - 15.3 Algorithm
  - 15.4 Developing and Evaluating an Anomaly Detection System
  - 15.5 Anomaly Detection vs. Supervised Learning
  - 15.6 Choosing What Features to Use
  - 15.7 Multivariate Gaussian Distribution (Optional)
  - 15.8 Anomaly Detection Using the Multivariate Gaussian Distribution (Optional)
+ Lecture 16 - Recommender Systems &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/16.pdf)]
  - 16.1 Problem Formulation
  - 16.2 Content Based Recommendations
  - 16.3 Collaborative Filtering
  - 16.4 Collaborative Filtering Algorithm
  - 16.5 Vectorization - Low Rank Matrix Factorization
  - 16.6 Implementation Detail - Mean Normalization
+ Lecture 17 - Large Scale Machine Learning &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/17.pdf)]
  - 17.1 Learning with Large Datasets
  - 17.2 Stochastic Gradient Descent
  - 17.3 Mini-Batch Gradient Descent
  - 17.4 Stochastic Gradient Descent - Convergence
  - 17.5 Online Learning
  - 17.6 MapReduce and Data Parallelism
+ Lecture 18 - Application Example: Photo OCR &nbsp;
[[Slides]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/slides/18.pdf)]
  - 18.1 Problem Description and Pipeline
  - 18.2 Sliding Windows
  - 18.3 Getting Lots of Data and Artificial Data
  - 18.4 Ceiling Analysis - What Part of Pipeline to Work on Next

<br>

## Exercises
+ Exercise 1: Linear Regression<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex1.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex1)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex1.ipynb)]
+ Exercise 2: Logistic Regression<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex2.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex2)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex2.ipynb)]
+ Exercise 3: Multi-class Classification and Neural Networks<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex3.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex3)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex3.ipynb)]
+ Exercise 4: Neural Networks Learning<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex4.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex4)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex4.ipynb)]
+ Exercise 5: Regularized Linear Regression and Bias vs. Variance<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex5.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex5)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex5.ipynb)]
+ Exercise 6: Support Vector Machines<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex6.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex6)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex6.ipynb)]
+ Exercise 7: K-means Clustering and Principal Component Analysis<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex7.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex7)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex7.ipynb)]
+ Exercise 8: Anomaly Detection and Recommender Systems<br>
[[Description]({{site.baseurl}}/algorithms/machinelearning/course/ml_andrewng/exercises/ex8.pdf)] &nbsp;
[[Data](https://github.com/shevapato2008/MLCourse_AndrewNg/tree/master/mlclass-ex8)] &nbsp;
[[Solution (ipynb)](https://github.com/shevapato2008/machine-learning-notebooks/blob/master/ml-ex8.ipynb)]

<br><br>
