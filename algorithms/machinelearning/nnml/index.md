---
layout: nnml
title: Neural Networks for Machine Learning (Geoffrey Hinton, University of Toronto)
comments: true
mathjax: false
---

## Syllabus

<br>

**Week 1 - Introduction**<br>
a. Why do We Need Machine Learning?<br>
b. What are Neural Networks?<br>
c. Some Simple Models of Neurons<br>
d. A Simple Example of Learning<br>
e. Three Types of Learning<br>

[Slides 01]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week1.pdf "Week 1: Introduction")<br>
[Quiz 01]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz01 "Quiz 1")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz01/quiz01.zip "quiz01.zip")]<br>

<br>

**Week 2 - [The Perceptron Learning Procedure]({{site.baseurl}}/algorithms/machinelearning/nnml/week2)**<br>
a. An Overview of the Main Types of Neural Network Architecture<br>
b. Perceptrons: The First Generation of Neural Networks<br>
c. A Geometrical View of Perceptrons<br>
d. Why the Learning Works?<br>
e. What Perceptrons Can't Do?<br>

[Slides 02]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week2.pdf "Week 2 Slides")<br>
[Quiz 02]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz02 "Quiz 2")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz02/quiz02.zip "quiz02.zip")]<br>

<br>

**Week 3 - [The Backpropagation Learning Procedure]({{site.baseurl}}/algorithms/machinelearning/nnml/week3)**<br>
a. Learning the Weights of A Linear Neuron<br>
b. The Error Surface for A Linear Neuron<br>
c. Learning the Weights of a Logistic Output Neuron<br>
d. The Backpropagation Algorithm<br>
e. How to Use the Derivatives Computed by the Backpropagation Algorithm<br>

[Slides 03]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week3.pdf "Week 3 Slides")<br>
[Quiz 03]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz03 "Quiz 3")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz03/quiz03.zip "quiz03.zip")]<br>
[Assignment 01]({{site.baseurl}}/algorithms/machinelearning/nnml/assignments/Assignment1/assign1.pdf "Assignment 1")
[[Code & Data](https://github.com/shevapato2008/Coursera_NNML_Hinton/tree/master/Assignments/Assignment1 "Github Repository")]

<br>

**Week 4 - [Learning Feature Vectors for Words]({{site.baseurl}}/algorithms/machinelearning/nnml/week4)**<br>
a. Learning to Predict the Next Word<br>
b. A Brief Diversion into Cognitive Science<br>
c. Another Diversion: The Softmax Output Function<br>
d. Neuro-Probabilistic Language Models<br>
e. Ways to Deal with the Large Number of Possible Outputs<br>

[Slides 04]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week4.pdf "Week 4 Slides")<br>
[Quiz 04]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz04 "Quiz 4")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz04/quiz04.zip "quiz04.zip")]<br>

<br>

**Week 5 - [Object Recognition with Neural Nets]({{site.baseurl}}/algorithms/machinelearning/nnml/week5)**<br>
a. Things that Make it Hard to Recognize Objects<br>
b. Ways to Achieve Viewpoint Invariance<br>
c. Convolutional Neural Networks for Hand-Written Digit Recognition<br>
d. Convolutional Neural Networks for Object Recognition<br>

[Slides 05]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week5.pdf "Week 5 Slides")<br>
[Quiz 05]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz05 "Quiz 5")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz05/quiz05.zip "quiz05.zip")]<br>
[Assignment 02]({{site.baseurl}}/algorithms/machinelearning/nnml/assignments/Assignment2/ "Assignment 2")
[[Code & Data](https://github.com/shevapato2008/Coursera_NNML_Hinton/tree/master/Assignments/Assignment2 "Github Repository")]

<br>

**Week 6 - [Optimization: How to Make the Learning Go Faster]({{site.baseurl}}/algorithms/machinelearning/nnml/week6)**<br>
a. Overview of Mini-Batch Gradient Descent<br>
b. A Bag of Tricks for Mini-Batch Gradient Descent<br>
c. The Momentum Method<br>
d. Adaptive Learning Rates for Each Connection<br>
e. Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude<br>

[Slides 06]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week6.pdf "Week 6 Slides")<br>
[Quiz 06]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz06 "Quiz 6")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz06/quiz06.zip "quiz06.zip")]<br>

<br>

**Week 7 - [Recurrent Neural Networks]({{site.baseurl}}/algorithms/machinelearning/nnml/week7)**<br>
a. Modeling Sequences: A Brief Overview<br>
b. Training RNNs with Backpropagation<br>
c. A Toy Example of Training An RNN<br>
d. Why it is Difficult to Train an RNN<br>
e. Long-Term Short-Term Memory<br>

[Slides 07]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week7.pdf "Week 7 Slides")<br>
[Quiz 07]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz07 "Quiz 7")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz07/quiz07.zip "quiz07.zip")]<br>

<br>

**Week 8 - [More Recurrent Neural Networks]({{site.baseurl}}/algorithms/machinelearning/nnml/week8)**<br>
a. A Brief Overview of Hessian-Free Optimization<br>
b. Modeling Character Strings with Multiplicative Connections<br>
c. Learning to Predict the Next Character Using HF<br>
d. Echo State Networks<br>

[Slides 08]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week8.pdf "Week 8 Slides")<br>
[Quiz 08]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz08 "Quiz 8")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz08/quiz08.zip "quiz08.zip")]<br>

<br>

**Week 9 - [Ways to Make Neural Networks Generalize Better]({{site.baseurl}}/algorithms/machinelearning/nnml/week9)**<br>
a. Overview of Ways to Improve Generalization<br>
b. Limiting the Size of the Weights<br>
c. Using Noise as a Regularizer<br>
d. Introduction to the Full Bayesian Approach<br>
e. The Bayesian Interpretation of Weight Decay<br>
f. MacKay's Quick and Dirty Method of Setting Weight Costs<br>

[Slides 09]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week9.pdf "Week 9 Slides")<br>
[Quiz 09]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz09 "Quiz 9")
[[zip]({{site.baseurl}}/algorithms/machinelearning/nnml/quizzes/quiz09/quiz09.zip "quiz09.zip")]<br>

<br>

**Week 10 - [Combining Multiple Neural Networks to Improve Generalization]({{site.baseurl}}/algorithms/machinelearning/nnml/week10)**<br>
a. Why it Helps to Combine Models?<br>
b. Mixture Experts<br>
c. The Idea of Full Bayesian Learning<br>
d. Making Full Bayesian Learning Practical<br>
e. Dropout<br>

[Slides 10]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week10.pdf "Week 10 Slides")<br>

<br><br>
