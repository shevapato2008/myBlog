---
layout: algorithm
title: Neural Networks for Machine Learning (Geoffrey Hinton, University of Toronto)
comments: true
---

# Neural Networks for Machine Learning
by Professor Geoffrey Hinton, University of Toronto

<br>

## Week 1 - Introduction

[Slides]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week1.pdf "Week 1: Introduction")

<br>

## Week 2 - [The Perceptron Learning Procedure]({{site.baseurl}}/algorithms/machinelearning/nnml/week2)<br>
a. An Overview of the Main Types of Neural Network Architecture<br>
b. Perceptrons: The First Generation of Neural Networks<br>
c. A Geometrical View of Perceptrons<br>
d. Why the Learning Works?<br>
e. What Perceptrons Can't Do?<br>

[Slides]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week2.pdf "Week 2 Slides")

## Week 3 - [The Backpropagation Learning Procedure]({{site.baseurl}}/algorithms/machinelearning/nnml/week3)<br>

a. Learning the Weights of A Linear Neuron<br>
b. The Error Surface for A Linear Neuron<br>
c. Learning the Weights of a Logistic Output Neuron<br>
d. The Backpropagation Algorithm<br>
e. How to Use the Derivatives Computed by the Backpropagation Algorithm<br>

[Slides]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week3.pdf "Week 3 Slides")

## Week 4 - [Learning Feature Vectors for Words]({{site.baseurl}}/algorithms/machinelearning/nnml/week4)<br>

a. Learning to Predict the Next Word<br>
b. A Brief Diversion into Cognitive Science<br>
c. Another Diversion: The Softmax Output Function<br>
d. Neuro-Probabilistic Language Models<br>
e. Ways to Deal with the Large Number of Possible Outputs<br>

[Slides]({{site.baseurl}}/algorithms/machinelearning/nnml/slides/Week4.pdf "Week 4 Slides")



<br><br>
